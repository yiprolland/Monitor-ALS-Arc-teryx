#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ALS.com Arc'teryx ç›‘æ§ï¼ˆç¨³å®šé”® = PDP slug / å•å“å•æ¡é€šçŸ¥ / åŸºçº¿ä¿æŠ¤ï¼‰
ç›‘æ§ï¼š
  1) ä¸Šæ–°ï¼ˆæ–°å•†å“/æ–°å˜ä½“ï¼‰
  2) ä»·æ ¼å˜åŒ–
  3) ä»…æé†’â€œç¼ºè´§ â†’ åˆ°è´§â€
  4) åº“å­˜æ•°é‡å¢åŠ ï¼ˆé€å°ºç å¯¹æ¯”ï¼›è§£æä¸åˆ°æ•°é‡åˆ™ 0/1 è¿‘ä¼¼ï¼‰

è¡Œä¸ºï¼š
  - åªé€šçŸ¥æœ‰å˜åŒ–çš„å•†å“
  - ä¸€ä¸ªå•†å“ä¸€æ¡é€šçŸ¥ï¼ˆåŒä¸€å•†å“çš„å¤šç§å˜åŒ–åˆå¹¶ä¸ºä¸€æ¡ï¼‰
  - ä½¿ç”¨ PDP è·¯å¾„ä¸­çš„ slug ä½œä¸ºç¨³å®š keyï¼Œé¿å…æ ‡é¢˜/SKU/é¢œè‰²æŠ–åŠ¨
  - åŸºçº¿ä¿æŠ¤ï¼šå½“â€œä¸Šæ–°å æ¯” > 70%â€æ—¶ï¼Œè·³è¿‡å½“æ¬¡â€œä¸Šæ–°â€é€šçŸ¥ï¼ˆå¯é€šè¿‡ BASELINE_PROTECT=0 å…³é—­ï¼‰

Env:
  DISCORD_WEBHOOK_URL   å¿…å¡«ï¼šDiscord Webhook
  HEADLESS=0/1          å¯é€‰ï¼šæœ¬åœ°è°ƒè¯• 0ï¼ŒCI 1ï¼ˆé»˜è®¤ 1ï¼‰
  KEYWORD_FILTER        å¯é€‰ï¼šä»…ç›‘æ§æ ‡é¢˜åŒ…å«è¯¥å…³é”®è¯ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼‰
  BASELINE_PROTECT=0/1  å¯é€‰ï¼šé»˜è®¤ 1ï¼Œå¼€å¯â€œä¸Šæ–°å æ¯”å¼‚å¸¸æ—¶æŠ‘åˆ¶ä¸Šæ–°é€šçŸ¥â€
  NOTIFY_INTERVAL_SEC   å¯é€‰ï¼šå•æ¡é€šçŸ¥é—´éš”ï¼Œé»˜è®¤ 0.1 ç§’
"""

import json
import os
import re
import sys
import time
import math
import random
import shutil
from datetime import datetime, timezone
from pathlib import Path
from tempfile import NamedTemporaryFile
from typing import Dict, Any, List, Tuple, Set
from urllib.parse import urlparse

from playwright.sync_api import sync_playwright, TimeoutError as PWTimeout

COLLECTION_URL = "https://www.als.com/arc-teryx"
SNAPSHOT_PATH = Path("snapshot.json")
USER_AGENT = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 "
    "(KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36"
)

# --------------------------
# Utilities
# --------------------------

def jdump(obj: Any, path: Path) -> None:
    """Atomic write to avoid half-written or empty JSON."""
    path.parent.mkdir(parents=True, exist_ok=True)
    with NamedTemporaryFile('w', delete=False, encoding='utf-8', dir=str(path.parent)) as tmp:
        json.dump(obj, tmp, ensure_ascii=False, indent=2)
        tmp.flush()
        os.fsync(tmp.fileno())
        tmp_name = tmp.name
    try:
        shutil.move(tmp_name, path)
    finally:
        try:
            os.unlink(tmp_name)
        except Exception:
            pass


def jload(path: Path) -> Dict[str, Any]:
    if not path.exists():
        print(f"[snapshot] {path} not found.")
        return {}
    try:
        data = json.loads(path.read_text(encoding="utf-8"))
        print(f"[snapshot] loaded {len(data)} items from {path}.")
        # æ‰“å°å‡ ä¸ªç¤ºä¾‹ keyï¼Œæ–¹ä¾¿éªŒè¯
        for i, k in enumerate(list(data.keys())[:5]):
            print(f"[snapshot] sample key {i+1}: {k}")
        return data
    except Exception as e:
        print(f"[snapshot] failed to parse {path}: {e}")
        return {}


def safe_sleep(a: float = 0.08, b: float = 0.22) -> None:
    time.sleep(random.uniform(a, b))


def now_iso() -> str:
    return datetime.now(timezone.utc).isoformat()


def norm_spaces(s: str) -> str:
    return re.sub(r"\s+", " ", s or "").strip()


def slug_from_pdp_url(u: str) -> str:
    """
    ä» PDP URL å– slugï¼š/arcteryx-xxx/p -> arcteryx-xxx
    """
    try:
        p = urlparse(u)
        path = (p.path or "").lower()
        m = re.search(r"/([^/]+)/p(?:$|/|\?|#)", path)
        return m.group(1) if m else path.strip("/").split("/")[-1]
    except Exception:
        u2 = (u or "").split("?")[0].split("#")[0].lower()
        return u2.strip("/").split("/")[-1]


def stable_key_from_url(u: str) -> str:
    """
    ç¨³å®š key ä»…ç”¨ slugï¼Œé¿å…æ ‡é¢˜/é¢œè‰²/SKU æŠ–åŠ¨å¯¼è‡´é‡å¤â€œä¸Šæ–°â€ã€‚
    åŒä¸€ PDPï¼ˆå³åŒä¸€è·¯ç”±ï¼‰â†’ åŒä¸€ keyã€‚
    """
    slug = slug_from_pdp_url(u)
    return slug or (u or "").lower()


# --------------------------
# Scraper
# --------------------------

def extract_collection_links(page) -> List[str]:
    """æ”¶é›†é›†åˆé¡µä¸Šçš„ PDP é“¾æ¥ã€‚"""
    anchors = page.locator("a[href*='/arcteryx-'][href*='/p']")
    hrefs = anchors.evaluate_all("els => els.map(e => e.href)")
    uniq: List[str] = []
    for h in hrefs:
        if "als.com" in h:
            h = h.split("#")[0]
            if h not in uniq:
                uniq.append(h)
    return uniq


def extract_sku(page) -> str:
    """è§£æ SKU / Style numberã€‚"""
    try:
        txt = page.locator("body").inner_text()
        m = re.search(r"(X\d{9,12})", txt)
        if m:
            return m.group(1).strip()
        m = re.search(r"(?:SKU|Style|Model)\s*[:#]\s*([A-Za-z0-9\-]+)", txt, re.I)
        if m:
            return m.group(1).strip()
    except Exception:
        pass
    # ç»“æ„åŒ–æ•°æ®å…œåº•
    try:
        metas = page.locator("script[type='application/ld+json']")
        for i in range(min(8, metas.count())):
            raw = metas.nth(i).inner_text()
            if not raw.strip():
                continue
            obj = json.loads(raw)
            if isinstance(obj, dict):
                sku = obj.get("sku")
                if sku:
                    return str(sku).strip()
            elif isinstance(obj, list):
                for it in obj:
                    if isinstance(it, dict) and it.get("sku"):
                        return str(it["sku"]).strip()
    except Exception:
        pass
    return ""


def extract_color(page) -> str:
    """è§£æé¢œè‰²ã€‚"""
    try:
        matches = page.locator("text=/Color\\s*:/i")
        if matches.count():
            line = matches.first.evaluate("el => el.parentElement ? el.parentElement.innerText : el.innerText")
            if line:
                m = re.search(r"Color\s*:\s*(.+)", line, re.I)
                if m:
                    return norm_spaces(m.group(1))
    except Exception:
        pass
    try:
        selected = page.locator("[aria-pressed='true'], [aria-selected='true']")
        for i in range(min(selected.count(), 8)):
            t = norm_spaces(selected.nth(i).inner_text())
            if t and len(t) <= 40 and not re.search(r"(Add to cart|Add to bag)", t, re.I):
                return t
    except Exception:
        pass
    try:
        if page.locator("h1").count():
            title = page.locator("h1").first.inner_text()
            m = re.search(r"\(([^()]+)\)$", title)
            if m:
                return norm_spaces(m.group(1))
    except Exception:
        pass
    return ""


def money_from_text(txt: str) -> Tuple[str, float]:
    """
    æŠ½å–è´§å¸ç¬¦å·ä¸é‡‘é¢ï¼Œä¾‹å¦‚ '$ 360.00' æˆ– 'CA$ 360'ã€‚
    è¿”å› (currency_symbol, price_float)ï¼›è‹¥å¤±è´¥ price=nan, symbol=''
    """
    if not txt:
        return "", math.nan
    m = re.search(r"([A-Z]{2}\$|\$|C\$|CA\$|US\$|â‚¬|Â£|Â¥)\s*([0-9]+(?:\.[0-9]{2})?)", txt.replace(",", ""))
    if m:
        return m.group(1), float(m.group(2))
    m = re.search(r"([0-9]+(?:\.[0-9]{2})?)", txt.replace(",", ""))
    if m:
        return "", float(m.group(1))
    return "", math.nan


def extract_price(page) -> Tuple[str, float]:
    """è§£æè´§å¸ä¸ä»·æ ¼ã€‚"""
    for sel in [
        "[class*='price']",
        "[data-test*='price']",
        "div:has-text('$'), div:has-text('CA$'), div:has-text('US$'), div:has-text('â‚¬'), div:has-text('Â£'), div:has-text('Â¥')",
        "body",
    ]:
        try:
            if page.locator(sel).count():
                txt = page.locator(sel).first.inner_text()
                cur, pr = money_from_text(txt)
                if not math.isnan(pr):
                    return cur, pr
        except Exception:
            continue
    return "", math.nan


def extract_sizes_with_qty(page) -> Dict[str, int]:
    """
    è¿”å› {size_text: qty_int}
    è§£æé¡ºåºï¼š
      1) data-available-qty / data-inventory / data-qty / data-stock
      2) é¡µé¢è„šæœ¬ä¸­çš„ "size":"XL","inventory_quantity":3
      3) å›é€€ï¼šæŒ‰é’®å¯ç‚¹=1ï¼Œä¸å¯ç‚¹=0
    """
    sizes: Dict[str, int] = {}

    # 1) data-* å±æ€§
    try:
        btns = page.locator("button, [role='option'], [data-size]")
        for i in range(min(btns.count(), 150)):
            el = btns.nth(i)
            label = norm_spaces(el.inner_text()).upper()
            if not label or len(label) > 10:
                continue
            if not re.fullmatch(r"(XXS|XS|S|M|L|XL|XXL|XXXL|[\d]{1,2})", label, re.I):
                continue
            qty_attr = None
            for attr in ("data-available-qty", "data-inventory", "data-qty", "data-stock", "data-quantity"):
                v = el.get_attribute(attr)
                if v and re.fullmatch(r"-?\d+", v.strip()):
                    qty_attr = int(v.strip())
                    break
            if qty_attr is not None:
                sizes[label] = max(0, qty_attr)
    except Exception:
        pass

    # 2) è„šæœ¬ä¸­çš„ JSON
    if not sizes:
        try:
            scripts = page.locator("script")
            for i in range(min(12, scripts.count())):
                raw = scripts.nth(i).inner_text()
                if not raw or ("variant" not in raw.lower() and "inventory" not in raw.lower()):
                    continue
                for m in re.finditer(r'"size"\s*:\s*"(?P<size>[^"]+?)"[^}]*?"inventory[^"]*?"\s*:\s*(?P<qty>-?\d+)', raw, re.I | re.S):
                    sizes[m.group("size").upper()] = max(0, int(m.group("qty")))
        except Exception:
            pass

    # 3) å›é€€ï¼šå¯ç‚¹=1ï¼Œä¸å¯ç‚¹=0
    if not sizes:
        try:
            candidates = page.locator(
                "button:has-text('XXS'), button:has-text('XS'), button:has-text('S'), "
                "button:has-text('M'), button:has-text('L'), button:has-text('XL'), "
                "button:has-text('XXL'), button:has-text('XXXL')"
            )
            for i in range(candidates.count()):
                el = candidates.nth(i)
                label = norm_spaces(el.inner_text()).upper()
                if not label:
                    continue
                disabled = el.get_attribute("disabled")
                aria = el.get_attribute("aria-disabled")
                cls = (el.get_attribute("class") or "")
                sizes[label] = 0 if (disabled is not None or aria in ("true", "disabled") or "disabled" in cls) else 1
        except Exception:
            pass

    return sizes


def parse_product_detail(page) -> Dict[str, Any]:
    """è§£æ PDP æ‰€éœ€å­—æ®µã€‚"""
    data = {
        "title": "",
        "sku": "",
        "color": "",
        "currency": "",
        "price": math.nan,
        "sizes": {},       # {size: qty_int}
        "in_stock": False, # ä»»ä¸€å°ºç  qty>0 å³ True
    }

    try:
        if page.locator("h1").count():
            data["title"] = norm_spaces(page.locator("h1").first.inner_text())
        elif page.locator("title").count():
            data["title"] = norm_spaces(page.locator("title").first.inner_text())
    except Exception:
        pass

    try:
        data["sku"] = extract_sku(page)
    except Exception:
        pass

    try:
        data["color"] = extract_color(page)
    except Exception:
        pass

    try:
        cur, pr = extract_price(page)
        data["currency"] = cur
        data["price"] = pr
    except Exception:
        pass

    try:
        sizes = extract_sizes_with_qty(page)
        data["sizes"] = sizes
        data["in_stock"] = any(qty > 0 for qty in sizes.values()) if sizes else False
    except Exception:
        pass

    return data


def scrape_all_products(headless: bool = True, timeout_ms: int = 8000) -> Dict[str, Any]:
    """éå†é›†åˆé¡µ â†’ é€ä¸ª PDP è§£æ â†’ è¿”å›ä»¥ ç¨³å®š slug ä¸ºé”® çš„ dictã€‚"""
    result: Dict[str, Any] = {}
    keyword = os.environ.get("KEYWORD_FILTER", "").strip().lower()

    with sync_playwright() as pw:
        browser = pw.chromium.launch(headless=headless, args=["--disable-http-cache"])
        ctx = browser.new_context(user_agent=USER_AGENT, locale="en-US")
        ctx.set_default_timeout(timeout_ms)

        # æ‹¦æˆªéå¿…è¦èµ„æºä»¥æé€Ÿ
        def _route(route):
            req = route.request
            if req.resource_type in ("image", "media", "font", "stylesheet"):
                return route.abort()
            return route.continue_()
        ctx.route("**/*", _route)

        page = ctx.new_page()

        page_idx = 1
        empty_hits = 0
        seen_urls: Set[str] = set()

        while True:
            url = COLLECTION_URL if page_idx == 1 else f"{COLLECTION_URL}?page={page_idx}"
            try:
                page.goto(url)
                page.wait_for_load_state("domcontentloaded")
            except PWTimeout:
                print(f"[page] timeout loading {url}")
                empty_hits += 1
                if empty_hits >= 2:
                    break
                page_idx += 1
                continue

            links = extract_collection_links(page)
            print(f"[collection] page {page_idx} links: {len(links)}")

            if not links:
                empty_hits += 1
                if empty_hits >= 2:
                    break
                page_idx += 1
                continue

            empty_hits = 0
            for href in links:
                if href in seen_urls:
                    continue
                seen_urls.add(href)
                safe_sleep()

                ok = False
                final_url = href
                for attempt in range(2):  # å°‘é‡é‡è¯•ä»¥æé€Ÿ
                    try:
                        page.goto(href)
                        page.wait_for_load_state("domcontentloaded")
                        safe_sleep()
                        final_url = page.url  # å–æœ€ç»ˆè·³è½¬åçš„ URL
                        pdata = parse_product_detail(page)
                        title = pdata.get("title", "")
                        if keyword and keyword not in (title or "").lower():
                            ok = True
                            break

                        # ç¨³å®š key & å±•ç¤º URL
                        slug = slug_from_pdp_url(final_url)
                        key = stable_key_from_url(final_url)
                        display_url = f"https://www.als.com/{slug}/p" if slug else final_url.split("?")[0].split("#")[0]

                        if title:
                            pdata.update({"url": display_url, "last_seen": now_iso(), "key": key})
                            result[key] = pdata
                            ok = True
                            break
                    except Exception as e:
                        print(f"[detail] error {href}: {e}")
                        safe_sleep(0.2, 0.4)

                if not ok:
                    slug = slug_from_pdp_url(final_url or href)
                    key = stable_key_from_url(final_url or href)
                    display_url = f"https://www.als.com/{slug}/p" if slug else (final_url or href).split("?")[0].split("#")[0]
                    result[key] = {
                        "title": "",
                        "sku": "",
                        "color": "",
                        "currency": "",
                        "price": math.nan,
                        "sizes": {},
                        "in_stock": False,
                        "url": display_url,
                        "last_seen": now_iso(),
                        "key": key,
                        "note": "parse_failed",
                    }
            page_idx += 1

        ctx.close()
        browser.close()

    return result


# --------------------------
# Diff & Notification
# --------------------------

def compute_diff(old: Dict[str, Any], new: Dict[str, Any]) -> Dict[str, Any]:
    """
    è¿”å›ï¼š
      new_items:        [(k, n)]
      price_changes:    [(k, o, n)]
      restocks:         [(k, o, n)]
      stock_increases:  [(k, o, n, increased_sizes_dict)]
    """
    new_items: List[Tuple[str, Dict[str, Any]]] = []
    price_changes: List[Tuple[str, Dict[str, Any], Dict[str, Any]]] = []
    restocks: List[Tuple[str, Dict[str, Any], Dict[str, Any]]] = []
    stock_increases: List[Tuple[str, Dict[str, Any], Dict[str, Any], Dict[str, int]]] = []

    old_keys = set(old.keys())
    new_keys = set(new.keys())

    # ä¸Šæ–°ï¼ˆå«æ–°å˜ä½“ï¼‰
    for k in sorted(new_keys - old_keys):
        new_items.append((k, new[k]))

    # äº¤é›†å¯¹æ¯”
    for k in sorted(new_keys & old_keys):
        o = old.get(k, {})
        n = new.get(k, {})

        # ä»·æ ¼å˜åŒ–
        op, np = o.get("price"), n.get("price")
        if (isinstance(op, (int, float)) and isinstance(np, (int, float))
                and not math.isnan(op) and not math.isnan(np) and abs(op - np) >= 0.01):
            price_changes.append((k, o, n))

        # ç¼ºè´§â†’åˆ°è´§ï¼ˆä»…æé†’è¿™ä¸€æ–¹å‘ï¼‰
        if (not o.get("in_stock", False)) and n.get("in_stock", False):
            restocks.append((k, o, n))

        # åº“å­˜æ•°é‡å¢åŠ ï¼ˆé€å°ºç ï¼‰
        increased: Dict[str, int] = {}
        osizes: Dict[str, int] = o.get("sizes") or {}
        nsizes: Dict[str, int] = n.get("sizes") or {}
        for size, nqty in nsizes.items():
            oqty = osizes.get(size, 0)
            try:
                if int(nqty) > int(oqty):
                    increased[size] = int(nqty)
            except Exception:
                if (nqty and not oqty):
                    increased[size] = 1
        if increased:
            stock_increases.append((k, o, n, increased))

    return {
        "new_items": new_items,
        "price_changes": price_changes,
        "restocks": restocks,
        "stock_increases": stock_increases,
    }


def _fmt_currency_price(currency: str, price: float) -> str:
    if isinstance(price, (int, float)) and not math.isnan(price):
        cur = (currency or "").strip()
        return f"{cur} {price:.2f}".strip() if cur else f"{price:.2f}"
    return "N/A"


def _fmt_sizes_line(sizes: Dict[str, int], only_keys: List[str] = None, limit: int = 8) -> str:
    items: List[str] = []
    if only_keys:
        for k in only_keys:
            if k in sizes:
                items.append(f"{k}:{sizes[k]}")
    else:
        for k, v in sizes.items():
            if v and v > 0:
                items.append(f"{k}:{v}")
                if len(items) >= limit:
                    break
    return "ï¼Œ".join(items) if items else "æ— "


def build_item_message(n: Dict[str, Any], reasons: List[str], increased_sizes: List[str] = None) -> Dict[str, Any]:
    """
    ä¸ºå•ä¸ªå•†å“æ„å»º Discord payloadï¼ˆä¸€ä¸ªå•†å“ä¸€æ¡æ¶ˆæ¯ï¼‰ã€‚
    reasons: ["ä¸Šæ–°", "ä»·æ ¼å˜åŒ–", "ç¼ºè´§â†’åˆ°è´§", "åº“å­˜å¢åŠ "]
    increased_sizes: å½“åŒ…å«â€œåº“å­˜å¢åŠ â€æ—¶ï¼Œä»…å±•ç¤ºå¢é•¿çš„å°ºç ï¼ˆå¯é€‰ï¼‰ã€‚
    """
    nm = n.get("title") or "-"
    sku = n.get("sku") or "-"
    color = n.get("color") or "-"
    price = _fmt_currency_price(n.get("currency", ""), n.get("price"))
    sizes = n.get("sizes") or {}

    if increased_sizes:
        sizes_line = _fmt_sizes_line(sizes, only_keys=increased_sizes)
    else:
        sizes_line = _fmt_sizes_line(sizes)

    header = "ã€".join(reasons)
    content = "\n".join([
        f"**{header}**",
        f"â€¢ åç§°ï¼š{nm}",
        f"â€¢ è´§å·ï¼š{sku}",
        f"â€¢ é¢œè‰²ï¼š{color}",
        f"â€¢ ä»·æ ¼ï¼š{price}",
        f"ğŸ§¾ åº“å­˜ä¿¡æ¯ï¼š{sizes_line}",
        f"{n.get('url')}",
    ])

    return {
        "content": None,
        "embeds": [{
            "title": "Al's | Arc'teryx ç›‘æ§",
            "description": content[:4000],
            "timestamp": datetime.utcnow().isoformat(),
            "color": 0x00AAFF,
            "footer": {"text": "als.com ä»·æ ¼/ä¸Šæ–°/åº“å­˜ç›‘æ§"},
        }]
    }


def send_discord(payload: dict) -> None:
    """
    Discord Webhook é€šçŸ¥ï¼šä»…å¿…è¦è¯·æ±‚å¤´ï¼›å•æ¬¡å‘é€ï¼ˆå¤±è´¥è·³è¿‡ï¼‰ï¼›è½»å¾®å‘é€é—´éš”é¿å… 429ã€‚
    ï¼ˆä¸å¸¦ Origin/Refererï¼Œè§„é¿ 50067 Invalid request originï¼‰
    """
    import urllib.request
    import urllib.error

    webhook = os.environ.get("DISCORD_WEBHOOK_URL", "").strip()
    if not webhook:
        print("WARN: DISCORD_WEBHOOK_URL æœªé…ç½®ï¼Œè·³è¿‡é€šçŸ¥ã€‚")
        return

    webhook = webhook.replace("discordapp.com", "discord.com")
    if "?" not in webhook:
        webhook = webhook + "?wait=true"

    data = json.dumps(payload).encode("utf-8")
    headers = {
        "Content-Type": "application/json",
        "Accept": "application/json",
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/122.0 Safari/537.36"
        ),
    }

    try:
        req = urllib.request.Request(webhook, data=data, headers=headers, method="POST")
        with urllib.request.urlopen(req, timeout=7) as resp:
            body = resp.read().decode("utf-8", "ignore")
            print(f"Discord OK: {resp.status} {body[:120]}")
    except urllib.error.HTTPError as e:
        body = e.read().decode("utf-8", "ignore")
        print(f"Discord HTTPError: {e.code} {body[:200]}")
    except Exception as ex:
        print(f"Discord error: {repr(ex)}")

    # å‘é€é—´éš”ï¼Œé¿å…é¢‘ç¹ 429ï¼ˆå¯æŒ‰éœ€è°ƒå¤§ï¼‰
    time.sleep(float(os.environ.get("NOTIFY_INTERVAL_SEC", "0.1")))


# --------------------------
# Main
# --------------------------

def main() -> int:
    print(f"CWD={os.getcwd()}  SNAPSHOT_PATH={SNAPSHOT_PATH.resolve()}")
    headless = os.environ.get("HEADLESS", "1") != "0"
    baseline_protect = os.environ.get("BASELINE_PROTECT", "1") != "0"

    old = jload(SNAPSHOT_PATH)
    print(f"Loaded {len(old)} items from snapshot.")

    # æŠ“å–
    new = scrape_all_products(headless=headless)
    print(f"Scraped {len(new)} items from website.")

    # è®¡ç®—å·®å¼‚ â†’ â€œä¸€å•†å“ä¸€æ¡â€èšåˆ
    diffs = compute_diff(old, new)

    reasons_map: Dict[str, List[str]] = {}
    increased_sizes_map: Dict[str, List[str]] = {}

    for k, n in diffs["new_items"]:
        reasons_map.setdefault(k, []).append("ä¸Šæ–°")
    for k, o, n in diffs["price_changes"]:
        reasons_map.setdefault(k, []).append("ä»·æ ¼å˜åŒ–")
    for k, o, n in diffs["restocks"]:
        reasons_map.setdefault(k, []).append("ç¼ºè´§â†’åˆ°è´§")
    for k, o, n, inc in diffs["stock_increases"]:
        reasons_map.setdefault(k, []).append("åº“å­˜å¢åŠ ")
        increased_sizes_map[k] = list(inc.keys())

    changed_keys = sorted(set(reasons_map.keys()))
    print("Changed items:", len(changed_keys))

    # --- åŸºçº¿ä¿æŠ¤ï¼šå¦‚æœâ€œä¸Šæ–°å æ¯”å¼‚å¸¸é«˜â€ï¼Œå½“æ¬¡ä¸ä¸Šå‘â€œä¸Šæ–°â€ ---
    if baseline_protect and changed_keys:
        total_new = len(diffs["new_items"])
        total_all = len(new) if new else 1
        ratio = total_new / total_all
        print(f"[baseline] new_ratio={ratio:.2%} (new={total_new}, all={total_all})")
        if ratio > 0.70 and total_all >= 20:  # æ ·æœ¬å°‘æ—¶ä¸è§¦å‘ä¿æŠ¤
            print("[baseline] too many NEW items detected; suppress NEW notifications for this run.")
            # ç§»é™¤â€œä¸Šæ–°â€åŸå› ï¼Œåªä¿ç•™ä»·æ ¼/åˆ°è´§/åº“å­˜å¢åŠ 
            for k, _n in list(diffs["new_items"]):
                if k in reasons_map:
                    reasons_map[k] = [r for r in reasons_map[k] if r != "ä¸Šæ–°"]
                    if not reasons_map[k]:
                        reasons_map.pop(k, None)
            changed_keys = sorted(set(reasons_map.keys()))
            print("Changed items after baseline protection:", len(changed_keys))

    # å†™å›å¿«ç…§ï¼ˆæ— è®ºæ˜¯å¦é€šçŸ¥ï¼‰
    jdump(new, SNAPSHOT_PATH)

    # é€æ¡é€šçŸ¥ï¼ˆåªå¯¹æœ‰å˜åŒ–çš„å•†å“ï¼‰
    if changed_keys:
        for k in changed_keys:
            n = new.get(k) or {}
            reasons = reasons_map.get(k, [])
            inc_sizes = increased_sizes_map.get(k)
            payload = build_item_message(n, reasons=reasons, increased_sizes=inc_sizes)
            send_discord(payload)
    else:
        print("No changes; no notifications.")

    return 0


if __name__ == "__main__":
    sys.exit(main())
